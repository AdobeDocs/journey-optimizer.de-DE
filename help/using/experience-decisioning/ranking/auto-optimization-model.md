---
product: experience platform
solution: Experience Platform
title: Modelle f√ºr die automatische Optimierung
description: Erfahren Sie mehr √ºber Modelle f√ºr die automatische Optimierung
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: 8a8b66cb-dd96-4373-bbe0-a67e0dc0b2c0
source-git-commit: 0e9d8335bed8d8157a0f2302a5ff2b0a74257218
workflow-type: tm+mt
source-wordcount: '1500'
ht-degree: 91%

---

# Modelle f√ºr die automatische Optimierung {#auto-optimization-model}

Mit einem Modell mit automatischer Optimierung werden Angebote geschaltet, die den von den Gesch√§ftskunden festgelegten Gewinn (KPIs) maximieren. Diese KPIs k√∂nnen in Form von Konversionsraten, Umsatz usw. vorliegen. Im Moment bezieht sich die automatische Optimierung auf die Optimierung von Angebotsklicks mit dem Ziel der Angebotskonvertierung. Die automatische Optimierung ist nicht personalisiert und erfolgt auf der Grundlage der ‚Äûglobalen‚Äú Performance der Angebote.

## Datensatzanforderungen

Um ein Modell mit automatischer Optimierung zu trainieren, muss der Datensatz die folgenden Mindestanforderungen erf√ºllen:

* Mindestens 2 Angebote im Datensatz m√ºssen innerhalb der letzten 14 Tage mindestens 100 Anzeigeereignisse und 5 Klickereignisse haben.
* Angebote mit weniger als 100 Anzeigen und/oder 5 Klickereignissen innerhalb der letzten 14 Tage werden vom Modell als neue Angebote behandelt und sind nur f√ºr die Bereitstellung durch den Explorationsbandit geeignet.
* Angebote mit mehr als 100 Anzeigen und 5 Klickereignissen in den letzten 14 Tagen werden vom Modell als vorhandene Angebote behandelt und k√∂nnen sowohl von Exploration- als auch von Exploitation-Banditen bedient werden.

Bis zum ersten Mal ein Modell f√ºr die automatische Optimierung trainiert wird, werden Angebote innerhalb einer Auswahlstrategie, die ein Modell f√ºr die automatische Optimierung verwendet, nach dem Zufallsprinzip bereitgestellt.

## Einschr√§nkungen {#limitations}

Die Verwendung von Modellen f√ºr die automatische Optimierung beim Entscheidungs-Management unterliegt den unten stehenden Einschr√§nkungen:

<!--* Auto-optimization models do not work with the Batch Decisioning API.-->
* Das zum Erstellen des Modells ben√∂tigte Feedback muss als Erlebnisereignis gesendet werden. Es sollte bei [!DNL Journey Optimizer]-Kan√§len nicht automatisch gesendet werden.

## Terminologie {#terminology}

Bei der automatisierten Optimierung sind die folgenden Begriffe hilfreich:

* **Mehrarmiger Bandit**:‚ÄØDer Ansatz [Mehrarmiger Bandit](https://de.wikipedia.org/wiki/Mehrarmiger_Bandit){target="_blank"} f√ºr die Optimierung bringt forschendes Lernen und die Nutzung des Erlernten miteinander in Einklang.

* **Thompson-Stichprobenverfahren**: Das Thompson-Stichprobenverfahren ist ein Algorithmus f√ºr Online-Entscheidungsprobleme, bei dem sequenziell Ma√ünahmen getroffen werden, die einen Ausgleich herstellen m√ºssen zwischen der Nutzung dessen, was bekannt ist, um die sofortige Performance zu maximieren, und Investitionen zur Sammlung neuer Informationen, die die zuk√ºnftige Performance verbessern k√∂nnen. [Weitere Informationen](#thompson-sampling)

* [**Beta-Verteilung**](https://de.wikipedia.org/wiki/Beta-Verteilung){target="_blank"}: Satz von kontinuierlichen [Wahrscheinlichkeitsverteilungen](https://de.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F){target="_blank"}, die im Intervall [0, 1] definiert und durch zwei positive [Formparameter](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"} [parametrisiert](https://de.wikipedia.org/wiki/Parameter_(Statistik)){target="_blank"} sind.

## Thompson-Stichprobenverfahren {#thompson-sampling}

Der Algorithmus, der der automatischen Optimierung zugrunde liegt, ist das **Thompson-Stichprobenverfahren**. In diesem Abschnitt besprechen wir die Idee hinter dem Thompson-Stichprobenverfahren.

Beim [Thompson-Stichprobenverfahren](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, oder bayessche Banditen, handelt es sich um einen bayesschen Ansatz f√ºr das Problem des mehrarmigen Banditen.  Die Grundidee besteht darin, die durchschnittliche Belohnung ùõç von jedem Angebot als **Zufallsvariable** zu behandeln und die bisher gesammelten Daten zu verwenden, um unsere ‚Äû√úberzeugung‚Äú √ºber die durchschnittliche Belohnung anzupassen. Diese ‚Äû√úberzeugung‚Äú wird mathematisch durch eine **A-posteriori-Wahrscheinlichkeitsverteilung** dargestellt¬†‚Äì im Prinzip ein Bereich von Werten f√ºr die durchschnittliche Belohnung gemeinsam mit der Plausibilit√§t (oder Wahrscheinlichkeit), dass die Belohnung diesen Wert f√ºr jedes Angebot hat.‚ÄØDanach entnehmen wir f√ºr jede Entscheidung **einen Punkt aus jeder dieser A-posteriori-Belohnungsverteilungen** und w√§hlen das Angebot aus, dessen Belohnung den h√∂chsten Wert hatte.

Dieser Vorgang wird in der folgenden Abbildung veranschaulicht, in der wir drei verschiedene Angebote haben. Anf√§nglich haben wir keine Erkenntnisse aus den Daten und nehmen an, dass alle Angebote eine einheitliche A-posteriori-Belohnungsverteilung haben. Wir ziehen eine Stichprobe aus der A-posteriori-Belohnungsverteilung eines jeden Angebots. Die aus der Verteilung von Angebot 2 ausgew√§hlte Stichprobe hat den h√∂chsten Wert. Dies ist ein Beispiel f√ºr eine **Exploration**. Nach der Anzeige von Angebot 2 erfassen wir jede potenzielle Belohnung (z. B. Konversion/Keine Konversion) und aktualisieren die A-posteriori-Verteilung von Angebot 2 mithilfe des Satzes von Bayes, wie unten beschrieben.  Wir setzen diesen Prozess fort und aktualisieren die A-posteriori-Verteilungen jedes Mal, wenn ein Angebot angezeigt und die Belohnung erfasst wird. In der zweiten Abbildung wird Angebot 3 ausgew√§hlt. Obwohl Angebot 1 die h√∂chste durchschnittliche Belohnung hat (die A-posteriori-Belohnungsverteilung ist am weitesten rechts), hat der Prozess der Stichprobenziehung aus jeder Verteilung dazu gef√ºhrt, dass wir das scheinbar suboptimale Angebot 3 ausgew√§hlt haben. Damit geben wir uns die M√∂glichkeit, mehr √ºber die wahre Belohnungsverteilung von Angebot 3 zu erfahren.

Je mehr Stichproben gesammelt werden, desto gr√∂√üer wird das Konfidenzniveau und desto genauer ist die erzielte Sch√§tzung der m√∂glichen Belohnung (entsprechend der engeren Belohnungsverteilungen).‚ÄØDieser Prozess der Aktualisierung unserer Annahmen durch die Verf√ºgbarkeit neuer Erkenntnisse wird als **Bayes&#39;sche Inferenz** bezeichnet.

Wenn ein Angebot (z. B. Angebot 1) ein eindeutiger Gewinner ist, wird seine A-posteriori-Belohnungsverteilung von anderen getrennt. Zu diesem Zeitpunkt ist die von Angebot 1 in die Stichprobe einbezogene Belohnung f√ºr jede Entscheidung wahrscheinlich die h√∂chste, und wir w√§hlen sie mit einer h√∂heren Wahrscheinlichkeit aus. Wir sprechen von **Exploitation** (Ausbeutung): Wir sind der festen √úberzeugung, dass Angebot 1 das beste ist, und daher wird es ausgew√§hlt, um die Belohnungen zu maximieren.

![](../assets/ai-ranking-thompson-sampling.png)

**Abbildung 1**: *F√ºr jede Entscheidung wird ein Punkt aus den A-posteriori-Belohnungsverteilungen entnommen. Das Angebot mit dem h√∂chsten Stichprobenwert (Konversionsrate) wird ausgew√§hlt. In der Anfangsphase haben alle Angebote eine einheitliche Verteilung, da wir aus den Daten keine Erkenntnisse zu den Konversionsraten der Angebote erhalten. Je mehr Stichproben wir sammeln, desto enger und genauer werden die A-posteriori-Verteilungen. Am Schluss wird jedes Mal das Angebot mit der h√∂chsten Konversionsrate ausgew√§hlt.*

+++**Technische Details**

Zur Berechnung/Aktualisierung der Verteilungen verwenden wir den **Satz von Bayes**. F√ºr jedes Angebot ***i*** m√∂chten wir sein ***P(ùõçi | Daten)*** berechnen, d.¬†h. f√ºr jedes Angebot ***i*** m√∂chten wir feststellen, wie wahrscheinlich der Belohnungswert **ùõçi** auf Basis der bisher f√ºr dieses Angebot gesammelten Daten ist.

Nach dem Satz von Bayes:

***A-posteriori = Wahrscheinlichkeit * A-priori***

Die **A-priori-Wahrscheinlichkeit** ist die anf√§ngliche Einsch√§tzung der Wahrscheinlichkeit, ein Ergebnis zu erzeugen. Die Wahrscheinlichkeit, nachdem einige Erkenntnisse gesammelt wurden, wird als **A-posteriori-Wahrscheinlichkeit** bezeichnet.‚ÄØ

Die automatische Optimierung ist so konzipiert, dass bin√§re Belohnungen (Klick/kein Klick) ber√ºcksichtigt werden. In diesem Fall stellt die Wahrscheinlichkeit die Anzahl der Erfolge aus n Versuchen dar und wird durch eine **Binomialverteilung** modelliert. Bei einigen Wahrscheinlichkeitsfunktionen befindet sich der Posterior, wenn Sie einen bestimmten Prior w√§hlen, in derselben Verteilung wie der Prior. Ein solcher Prior wird dann **konjugierter Priori** genannt. Diese Art von Prior macht die Berechnung der A-posteriori-Verteilung sehr einfach. Die **Beta-Verteilung** ist ein konjugierter Prior zur Binomialwahrscheinlichkeit (bin√§re Belohnungen) und daher eine bequeme und sinnvolle Wahl f√ºr die A-priori- und A-posteriori-Wahrscheinlichkeitsverteilungen. Die Beta-Verteilung hat zwei Parameter, ***Œ±*** und ***Œ≤***.‚ÄØDiese Parameter k√∂nnen als Anzahl von Erfolgen und Fehlschl√§gen betrachtet werden, und der Mittelwert ist gegeben durch:

![](../assets/ai-ranking-beta-distribution.png)

Die Wahrscheinlichkeitsfunktion wird, wie oben erl√§utert, durch eine Binomialverteilung modelliert, mit s Erfolgen (Konversionen) und f Misserfolgen (keine Konversionen), und q ist eine [Zufallsvariable](https://de.wikipedia.org/wiki/Zufallsvariable){target="_blank"} mit einer [Beta-Verteilung](https://de.wikipedia.org/wiki/Beta-Verteilung){target="_blank"}.

Der Prior wird von der Beta-Verteilung modelliert und die A-posteriori-Verteilung hat die folgende Form:

![](../assets/ai-ranking-posterior-distribution.svg)

Die Berechnung des Posteriors erfolgt einfach durch Addieren der Erfolge und Fehler zu den vorhandenen Parametern ***Œ±***, ***Œ≤***.

F√ºr die automatische Optimierung beginnen wir, wie im Beispiel oben gezeigt, mit der A-priori-Verteilung ***Beta(1, 1)*** (einheitliche Verteilung) f√ºr alle Angebote, und nach dem Erzielen von s Erfolgen und f Fehlschl√§gen f√ºr ein bestimmtes Angebot wird der Posterior zu einer Beta-Verteilung mit den Parametern‚ÄØ***(s+Œ±, f+Œ≤)*** f√ºr dieses Angebot.
+++

**Verwandte Themen**:

Um einen tieferen Einblick in das Thompson-Stichprobenverfahren zu erhalten, lesen Sie die folgenden Forschungsarbeiten:

* [Eine empirische Auswertung des Thompson-Stichprobenverfahrens](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [Analyse des Thompson-Stichprobenverfahrens f√ºr das Problem des mehrarmigen Banditen](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## ‚ÄûKaltstart‚Äú-Problem {#cold-start}

Das ‚ÄûKaltstart‚Äú-Problem tritt auf, wenn ein neues Angebot zu einer Kampagne hinzugef√ºgt wird und keine Daten zur Konversionsrate des neuen Angebots verf√ºgbar sind. W√§hrend dieses Zeitraums m√ºssen wir eine Strategie daf√ºr entwickeln, wie oft dieses neue Angebot ausgew√§hlt wird, damit der Performance-Abfall minimiert wird, w√§hrend wir Informationen √ºber die Konversionsrate dieses neuen Angebots sammeln. Es gibt mehrere L√∂sungen, um dieses Problem zu beheben. Wichtig dabei ist, ein Gleichgewicht zu finden, sodass die Nutzung dieses neuen Angebots nicht zu stark auf Kosten der Nutzung geht. Derzeit verwenden wir als anf√§ngliche Sch√§tzung der Konversionsrate des neuen Angebots (A-priori-Verteilung) eine ‚Äûeinheitliche Verteilung‚Äú. Das bedeutet, dass wir allen Konversionsratenwerten die gleiche Wahrscheinlichkeit zuweisen aufzutreten.

![](../assets/ai-ranking-cold-start-strategies.png)

**Abbildung 2**: *Nehmen wir eine Kampagne mit drei Angeboten an. W√§hrend der Kampagne wird Angebot 4 zur Kampagne hinzugef√ºgt. Zun√§chst liegen keine Daten √ºber die Konversionsrate von Angebot 4 vor, und wir sind mit dem Kaltstart-Problem konfrontiert. Wir verwenden eine gleichm√§√üige Verteilung als ersten Sch√§tzwert zur Konversionsrate von Angebot 4, w√§hrend wir Daten f√ºr dieses neue Angebot sammeln. Wie im Abschnitt [Thompson-Stichprobenverfahren](#thompson-sampling) erl√§utert, w√§hlen wir Stichproben aus den A-posteriori-Belohnungsverteilungen der Angebote und das Angebot mit dem h√∂chsten Stichprobenwert aus, um zu entscheiden, welches Angebot einem Nutzer angezeigt wird. Im obigen Beispiel wird Angebot 4 gew√§hlt und sp√§ter wird die A-posteriori-Verteilung dieses Angebots auf der Grundlage der erfassten Belohnungen aktualisiert, wie im Abschnitt [Thompson-Stichprobenverfahren](#thompson-sampling) erl√§utert.*

## Steigerungsmessung {#lift}

Mit der Metrik ‚ÄûSteigerung‚Äú wird die Performance einer Strategie im Rangfolgen-Service gegen√ºber einer Basisstrategie (d.¬†h. nur zuf√§llig bereitgestellte Angebote) gemessen.

Wenn wir z.¬†B. die Performance einer TS-Strategie (Thompson-Stichprobenverfahren) im Rangfolgen-Service messen m√∂chten und der KPI die Konversionsrate (CVR) ist, wird die ‚ÄûSteigerung‚Äú der TS-Strategie gegen√ºber der Basisstrategie folgenderma√üen definiert:

![](../assets/ai-ranking-lift.png)

>[!NOTE]
>
>Derzeit ist der Bericht zur Steigerungsmessung nur f√ºr das KI-Modell zur [personalisierten Optimierung](personalized-optimization-model.md) verf√ºgbar. [Weitere Informationen √ºber Berichte zur Entscheidungsfindung](../../reports/campaign-global-report-cja-code.md#decisioning-reporting)
