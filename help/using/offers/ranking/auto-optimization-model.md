---
product: experience platform
solution: Experience Platform
title: Modelle f√ºr die automatische Optimierung
description: Erfahren Sie mehr √ºber Modelle f√ºr die automatische Optimierung
feature: Ranking, Decision Management
role: User
level: Experienced
exl-id: a85de6a9-ece2-43da-8789-e4f8b0e4a0e7
source-git-commit: 07b1f9b885574bb6418310a71c3060fa67f6cac3
workflow-type: tm+mt
source-wordcount: '1336'
ht-degree: 100%

---

# Modelle f√ºr die automatische Optimierung {#auto-optimization-model}

Mit einem Modell mit automatischer Optimierung werden Angebote geschaltet, die den von den Gesch√§ftskunden festgelegten Gewinn (KPIs) maximieren. Diese KPIs k√∂nnen in Form von Konversionsraten, Umsatz usw. vorliegen. Im Moment bezieht sich die automatische Optimierung auf die Optimierung von Angebotsklicks mit dem Ziel der Angebotskonvertierung. Die automatische Optimierung ist nicht personalisiert und erfolgt auf der Grundlage der ‚Äûglobalen‚Äú Leistung der Angebote.

## Einschr√§nkungen {#limitations}

Die Verwendung von Modellen f√ºr die automatische Optimierung beim Entscheidungs-Management unterliegt den unten stehenden Einschr√§nkungen:

* Modelle f√ºr die automatische Optimierung funktionieren nicht mit der Batch Decisioning-API.
* Das zum Erstellen des Modells ben√∂tigte Feedback muss als Erlebnisereignis gesendet werden. Es sollte bei [!DNL Journey Optimizer]-Kan√§len nicht automatisch gesendet werden.

## Terminologie {#terminology}

Bei der automatisierten Optimierung sind die folgenden Begriffe hilfreich:

* **Mehrarmiger Bandit**:‚ÄØDer Ansatz [Mehrarmiger Bandit](https://de.wikipedia.org/wiki/Mehrarmiger_Bandit){target="_blank"} f√ºr die Optimierung bringt entdeckendes Lernen und die Nutzung des Erlernten miteinander in Einklang.

* **Thompson-Stichprobenverfahren**: Das Thompson-Stichprobenverfahren ist ein Algorithmus f√ºr Online-Entscheidungsprobleme, bei dem sequenziell Ma√ünahmen getroffen werden, die einen Ausgleich herstellen m√ºssen zwischen der Nutzung dessen, was bekannt ist, um die sofortige Leistung zu maximieren, und Investitionen zur Sammlung neuer Informationen, die die zuk√ºnftige Leistung verbessern k√∂nnen. [Weitere Informationen](#thompson-sampling)

* [**Beta-Verteilung**](https://de.wikipedia.org/wiki/Beta-Verteilung){target="_blank"}: Set of continuous‚ÄØ[probability distributions](https://de.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F){target="_blank"}‚ÄØdefined on the interval [0, 1] [parameterized](https://de.wikipedia.org/wiki/Parameter_(Statistik)){target="_blank"}‚ÄØby two positive‚ÄØ[shape parameters](https://en.wikipedia.org/wiki/Shape_parameter){target="_blank"}.

## Thompson-Stichprobenverfahren {#thompson-sampling}

Der Algorithmus, der der automatischen Optimierung zugrunde liegt, ist das **Thompson-Stichprobenverfahren**. In diesem Abschnitt besprechen wir die Idee hinter dem Thompson-Stichprobenverfahren.

Beim [Thompson-Stichprobenverfahren](https://en.wikipedia.org/wiki/Thompson_sampling){target="_blank"}, oder ‚ÄûBayes&#39;sche Banditen‚Äú, handelt es sich um einen Bayes&#39;schen Ansatz f√ºr das Problem des mehrarmigen Banditen.  Die Grundidee besteht darin, die durchschnittliche Belohnung von jedem Angebot als **Zufallsvariable** zu behandeln und die bisher gesammelten Daten zu verwenden, um unsere ‚Äû√úberzeugung‚Äú hinsichtlich der durchschnittlichen Belohnung zu aktualisieren. Diese ‚Äû√úberzeugung‚Äú wird mathematisch durch eine **A-posteriori-Wahrscheinlichkeitsverteilung** dargestellt¬†‚Äì im Prinzip ein Bereich von Werten f√ºr die durchschnittliche Belohnung gemeinsam mit der Plausibilit√§t (oder Wahrscheinlichkeit), dass die Belohnung diesen Wert f√ºr jedes Angebot hat.‚ÄØDanach entnehmen wir f√ºr jede Entscheidung **einen Punkt aus jeder dieser A-posteriori-Belohnungsverteilungen** und w√§hlen das Angebot aus, dessen Belohnung den h√∂chsten Wert hatte.

Dieser Vorgang wird in der folgenden Abbildung veranschaulicht, in der wir drei verschiedene Angebote haben. Anf√§nglich haben wir keine Erkenntnisse aus den Daten und nehmen an, dass alle Angebote eine einheitliche A-posteriori-Belohnungsverteilung haben. Wir ziehen eine Stichprobe aus der A-posteriori-Belohnungsverteilung eines jeden Angebots. Die aus der Verteilung von Angebot 2 ausgew√§hlte Stichprobe hat den h√∂chsten Wert. Dies ist ein Beispiel f√ºr eine **Exploration**. Nach der Anzeige von Angebot 2 erfassen wir jede potenzielle Belohnung (z. B. Konversion/Keine Konversion) und aktualisieren die A-posteriori-Verteilung von Angebot 2 mithilfe des Satzes von Bayes, wie unten beschrieben.  Wir setzen diesen Prozess fort und aktualisieren die A-posteriori-Verteilungen jedes Mal, wenn ein Angebot angezeigt und die Belohnung erfasst wird. In der zweiten Abbildung wird Angebot 3 ausgew√§hlt. Obwohl Angebot 1 die h√∂chste durchschnittliche Belohnung hat (die A-posteriori-Belohnungsverteilung ist am weitesten rechts), hat der Prozess der Stichprobenziehung aus jeder Verteilung dazu gef√ºhrt, dass wir das scheinbar suboptimale Angebot 3 ausgew√§hlt haben. Damit geben wir uns die M√∂glichkeit, mehr √ºber die wahre Belohnungsverteilung von Angebot 3 zu erfahren.

Je mehr Stichproben gesammelt werden, desto gr√∂√üer wird das Konfidenzniveau und desto genauer ist die erzielte Sch√§tzung der m√∂glichen Belohnung (entsprechend der engeren Belohnungsverteilungen).‚ÄØDieser Prozess der Aktualisierung unserer Annahmen durch die Verf√ºgbarkeit neuer Erkenntnisse wird als **Bayes&#39;sche Inferenz** bezeichnet.

Wenn ein Angebot (z. B. Angebot 1) ein eindeutiger Gewinner ist, wird seine A-posteriori-Belohnungsverteilung von anderen getrennt. Zu diesem Zeitpunkt ist die von Angebot 1 in die Stichprobe einbezogene Belohnung f√ºr jede Entscheidung wahrscheinlich die h√∂chste, und wir w√§hlen sie mit einer h√∂heren Wahrscheinlichkeit aus. Wir sprechen von **Exploitation** (Ausbeutung): Wir sind der festen √úberzeugung, dass Angebot 1 das beste ist, und daher wird es ausgew√§hlt, um die Belohnungen zu maximieren.

![](../assets/ai-ranking-thompson-sampling.png)

**Abbildung 1**: *F√ºr jede Entscheidung wird ein Punkt aus den A-posteriori-Belohnungsverteilungen entnommen. Das Angebot mit dem h√∂chsten Stichprobenwert (Konversionsrate) wird ausgew√§hlt. In der Anfangsphase haben alle Angebote eine einheitliche Verteilung, da wir aus den Daten keine Erkenntnisse √ºber die Konversionsraten der Angebote haben. Je mehr Stichproben wir sammeln, desto enger und genauer werden die A-posteriori-Verteilungen. Am Schluss wird jedes Mal das Angebot mit der h√∂chsten Konversionsrate ausgew√§hlt.*

<!--
![](../assets/ai-ranking-thompson-sampling-initial.png)
![](../assets/ai-ranking-thompson-sampling-intermediate.png)
![](../assets/ai-ranking-thompson-sampling-ultimate.png)
-->

+++**Technische Details**

Zur Berechnung/Aktualisierung der Verteilungen verwenden wir den **Satz von Bayes**. F√ºr jedes Angebot ***i*** m√∂chten wir sein ***P(ùõçi | data)*** berechnen, d. h. f√ºr jedes Angebot ***i*** m√∂chten wir feststellen, wie wahrscheinlich der Belohnungswert **ùõçi** auf der Basis der bisher f√ºr dieses Angebot gesammelten Daten ist.

Nach dem Satz von Bayes:

***A-posteriori = Wahrscheinlichkeit * A-priori***

Die **A-priori-Wahrscheinlichkeit** ist die anf√§ngliche Einsch√§tzung der Wahrscheinlichkeit, ein Ergebnis zu erzeugen. Die Wahrscheinlichkeit, nachdem einige Erkenntnisse gesammelt wurden, wird als **A-posteriori-Wahrscheinlichkeit** bezeichnet.‚ÄØ

Die automatische Optimierung ist so konzipiert, dass bin√§re Belohnungen (Klick/kein Klick) ber√ºcksichtigt werden. In diesem Fall stellt die Wahrscheinlichkeit die Anzahl der Erfolge aus n Versuchen dar und wird durch eine **Binomialverteilung** modelliert. Bei einigen Wahrscheinlichkeitsfunktionen befindet sich der Posterior, wenn Sie einen bestimmten Prior w√§hlen, in derselben Verteilung wie der Prior. Ein solcher Prior wird dann **konjugierter Priori** genannt. Diese Art von Prior macht die Berechnung der A-posteriori-Verteilung sehr einfach. Die **Beta-Verteilung** ist ein konjugierter Prior zur Binomialwahrscheinlichkeit (bin√§re Belohnungen) und daher eine bequeme und sinnvolle Wahl f√ºr die A-priori- und A-posteriori-Wahrscheinlichkeitsverteilungen. Die Beta-Verteilung hat zwei Parameter, ***Œ±*** und ***Œ≤***.‚ÄØDiese Parameter k√∂nnen als Anzahl von Erfolgen und Fehlschl√§gen betrachtet werden, und der Mittelwert ist gegeben durch:

![](../assets/ai-ranking-beta-distribution.png)

Die Wahrscheinlichkeitsfunktion wird, wie oben erl√§utert, durch eine Binomialverteilung modelliert, mit s Erfolgen (Konversionen) und f Misserfolgen (keine Konversionen), und q ist eine [Zufallsvariable](https://de.wikipedia.org/wiki/Zufallsvariable){target="_blank"} with a [beta distribution](https://de.wikipedia.org/wiki/Beta-Verteilung){target="_blank"}.

Der Prior wird von der Beta-Verteilung modelliert und die A-posteriori-Verteilung hat die folgende Form:

![](../assets/ai-ranking-posterior-distribution.svg)

Die Berechnung des Posteriors erfolgt einfach durch Addieren der Erfolge und Fehler zu den vorhandenen Parametern ***Œ±***, ***Œ≤***.

F√ºr die automatische Optimierung beginnen wir, wie im Beispiel oben gezeigt, mit der A-priori-Verteilung ***Beta(1, 1)*** (einheitliche Verteilung) f√ºr alle Angebote, und nach dem Erzielen von s Erfolgen und f Fehlschl√§gen f√ºr ein bestimmtes Angebot wird der Posterior zu einer Beta-Verteilung mit den Parametern‚ÄØ***(s+Œ±, f+Œ≤)*** f√ºr dieses Angebot.
+++

**Verwandte Themen**:

Um einen tieferen Einblick in das Thompson-Stichprobenverfahren zu erhalten, lesen Sie die folgenden Forschungsarbeiten:
* [Eine empirische Auswertung des Thompson-Stichprobenverfahrens](https://proceedings.neurips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf){target="_blank"}
* [Analyse des Thompson-Stichprobenverfahrens f√ºr das Mehrarmiger-Bandit-Problem](https://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf){target="_blank"}

## ‚ÄûKaltstart‚Äú-Problem {#cold-start}

Das ‚ÄûKaltstart‚Äú-Problem tritt auf, wenn ein neues Angebot zu einer Kampagne hinzugef√ºgt wird und keine Daten zur Konversionsrate des neuen Angebots verf√ºgbar sind. W√§hrend dieses Zeitraums m√ºssen wir eine Strategie daf√ºr entwickeln, wie oft dieses neue Angebot ausgew√§hlt wird, damit der Leistungsabfall minimiert wird, w√§hrend wir Informationen √ºber die Konversionsrate dieses neuen Angebots sammeln. Es gibt mehrere L√∂sungen, um dieses Problem zu beheben. Wichtig dabei ist, ein Gleichgewicht zu finden, sodass die Exploration dieses neuen Angebots nicht zu stark auf Kosten der Exploitation geht. Derzeit verwenden wir als anf√§ngliche Sch√§tzung der Konversionsrate des neuen Angebots (A-priori-Verteilung) eine ‚Äûeinheitliche Verteilung‚Äú. Das bedeutet, dass wir allen Konversionsratenwerten die gleiche Wahrscheinlichkeit zuweisen aufzutreten.


![](../assets/ai-ranking-cold-start-strategies.png)

**Abbildung 2**: *Nehmen wir eine Kampagne mit drei Angeboten an. W√§hrend der Kampagne wird Angebot 4 zur Kampagne hinzugef√ºgt. Zun√§chst liegen keine Daten √ºber die Konversionsrate von Angebot 4 vor, und wir sind mit dem Kaltstart-Problem konfrontiert. Wir verwenden eine gleichm√§√üige Verteilung als ersten Sch√§tzwert zur Konversionsrate von Angebot 4, w√§hrend wir Daten f√ºr dieses neue Angebot sammeln. Wie im Abschnitt [Thompson-Stichprobenverfahren](#thompson-sampling) erl√§utert, w√§hlen wir Stichproben aus den A-posteriori-Belohnungsverteilungen der Angebote und das Angebot mit dem h√∂chsten Stichprobenwert aus, um zu entscheiden, welches Angebot einem Nutzer angezeigt wird. Im obigen Beispiel wird Angebot 4 gew√§hlt und sp√§ter wird die A-posteriori-Verteilung dieses Angebots auf der Grundlage der erfassten Belohnungen aktualisiert, wie im Abschnitt [Thompson-Stichprobenverfahren](#thompson-sampling) erl√§utert.*

## Steigerungsmessung {#lift}

Mit der Metrik ‚ÄûSteigerung‚Äú wird die Leistung einer Strategie im Rangfolgen-Service gegen√ºber einer Basisstrategie (d.¬†h. nur zuf√§llig bereitgestellte Angebote) gemessen.

Wenn wir z.¬†B. die Performance einer TS-Strategie (Thompson-Stichprobenverfahren) im Rangfolgen-Service messen m√∂chten und der KPI die Konversionsrate (CVR) ist, wird die ‚ÄûSteigerung‚Äú der TS-Strategie gegen√ºber der Basisstrategie folgenderma√üen definiert:

![](../assets/ai-ranking-lift.png)
